# Copyright (c) Facebook, Inc. and its affiliates. All Rights Reserved
"""
PIDæ•°æ®é›†å¤„ç† - åŸºäºåŸå§‹ç‰ˆæœ¬æ¢å¤ï¼Œæ”¯æŒGraphMLæ ¼å¼æ ‡æ³¨å’ŒCOCOå…¼å®¹æ€§
"""
import os
import sys
import torch
import torchvision
import torchvision.transforms as T
import torchvision.datasets as datasets
import numpy as np
from torch.utils.data import Dataset, DataLoader, Subset
from PIL import Image, ImageFile
import networkx as nx
import random
import tempfile
import json
from pycocotools.coco import COCO
from .transforms import Compose, ToTensor, Normalize

# å…è®¸åŠ è½½è¢«æˆªæ–­çš„å›¾åƒ
ImageFile.LOAD_TRUNCATED_IMAGES = True


class PIDGraphDatasetExtractor:
    """PIDæ•°æ®é›†æå–å™¨ï¼Œå¤„ç†GraphMLæ ¼å¼çš„æ ‡æ³¨æ–‡ä»¶"""
    
    def __init__(self, base_dir):
        self.base_dir = base_dir
        self.data_info = []
        

    def extract(self):
        """æå–æ‰€æœ‰é…å¯¹çš„å›¾åƒå’Œæ ‡æ³¨æ–‡ä»¶"""
        paired_files = []
        for root, dirs, files in os.walk(self.base_dir):
            # æŒ‰æ–‡ä»¶ååˆ†ç»„
            file_groups = {}
            
            for file in files:
                base_name = os.path.splitext(file)[0]
                ext = os.path.splitext(file)[1].lower()
                
                if base_name not in file_groups:
                    file_groups[base_name] = {}
                
                if ext in ['.png', '.jpg', '.jpeg']:
                    file_groups[base_name]['image'] = os.path.join(root, file)
                elif ext == '.graphml':
                    file_groups[base_name]['annotation'] = os.path.join(root, file)
            
            # æ‰¾åˆ°å®Œæ•´é…å¯¹çš„æ–‡ä»¶
            for base_name, files_dict in file_groups.items():
                if 'image' in files_dict and 'annotation' in files_dict:
                    paired_files.append({
                        'id': base_name,
                        'image_path': files_dict['image'],
                        'annotation_path': files_dict['annotation'],
                        'relative_path': os.path.relpath(root, self.base_dir)
                    })

        print(f"åœ¨ {self.base_dir} ä¸­æ‰¾åˆ° {len(paired_files)} å¯¹é…å¯¹æ–‡ä»¶")
        return paired_files
    
    def extract_annotations_from_graphml(self, graphml_path):
        """ä» GraphML æ–‡ä»¶ä¸­æå–æ ‡æ³¨ä¿¡æ¯"""
        try:
            graph = nx.read_graphml(graphml_path)
            
            # æå–èŠ‚ç‚¹ï¼ˆå¯¹è±¡ï¼‰ä¿¡æ¯
            objects = []
            for node_id, node_attrs in graph.nodes(data=True):
                # æ£€æŸ¥æ˜¯å¦æœ‰è¾¹ç•Œæ¡†ä¿¡æ¯
                bbox_keys = ['xmin', 'ymin', 'xmax', 'ymax']
                if all(key in node_attrs for key in bbox_keys):
                    try:
                        bbox = [float(node_attrs[key]) for key in bbox_keys]
                        
                        label = node_attrs.get('label', 'unknown')
                        # è¿‡æ»¤èƒŒæ™¯ç±»
                        if label == 'background':
                            continue
                            
                        obj_info = {
                            'id': node_id,
                            'bbox': bbox,  # [xmin, ymin, xmax, ymax]
                            'label': label,
                            'category': node_attrs.get('category', 'object'),
                            'attributes': {k: v for k, v in node_attrs.items() 
                                         if k not in bbox_keys + ['label', 'category']}
                        }
                        objects.append(obj_info)
                    except (ValueError, TypeError):
                        continue
            
            # æå–è¾¹ï¼ˆå…³ç³»ï¼‰ä¿¡æ¯
            relations = []
            for src, dst, edge_attrs in graph.edges(data=True):
                rel_info = {
                    'subject': src,
                    'object': dst,
                    'predicate': edge_attrs.get('edge_label', 'unknown'),
                    'attributes': dict(edge_attrs)
                }
                relations.append(rel_info)
            
            return {
                'objects': objects,
                'relations': relations,
                'num_objects': len(objects),
                'num_relations': len(relations)
            }
            
        except Exception as e:
            print(f"è§£æ GraphML æ–‡ä»¶å¤±è´¥ {graphml_path}: {e}")
            return None


class PIDGraphDataset(Dataset):
    """PIDå›¾æ•°æ®é›†ç±»ï¼Œæ”¯æŒGraphMLæ ¼å¼çš„æ ‡æ³¨"""
    
    def __init__(self, complete_img=False, min_objects=1, transform=None, base_path='data/PID'):
        if complete_img == False:
            self.base_dir = os.path.join(base_path, 'patched')
        else:
            # ä¼˜å…ˆä½¿ç”¨è°ƒæ•´åˆ†è¾¨ç‡åçš„æ•°æ®ï¼ˆå…è®¸é€’å½’æŸ¥æ‰¾ï¼Œä¸å¼ºåˆ¶è¦æ±‚é¡¶å±‚æœ‰graphmlï¼‰
            resized_path = "/mnt/ShareDB_6TB/baitianyou/RelTR-main/data/pid_resized"
            if os.path.exists(resized_path):
                self.base_dir = resized_path
                print(f"ğŸ”§ ä½¿ç”¨è°ƒæ•´åˆ†è¾¨ç‡åçš„æ•°æ®: {resized_path}")
            # å…¶æ¬¡ä½¿ç”¨æä¾›çš„base_pathï¼ˆå…è®¸é€’å½’æŸ¥æ‰¾ï¼‰
            elif os.path.exists(base_path):
                self.base_dir = base_path  # ç›´æ¥ä½¿ç”¨åŒ…å«æ•°æ®çš„ç›®å½•æˆ–å…¶å­ç›®å½•
            else:
                self.base_dir = os.path.join(base_path, 'Complete')  # ä½¿ç”¨å­ç›®å½•
            
        self.transform = transform
        
        # 1. åŠ è½½å…³ç³»ç±»åˆ«é…ç½®ï¼ˆæ— èƒŒæ™¯ç±»åˆ«ï¼‰
        self.rel_categories = ['solid',            # 0 - å®çº¿è¿æ¥
                               'non-solid',       # 1 - è™šçº¿è¿æ¥
                               ]
        
        # 2. åˆ›å»ºå…³ç³»ç±»åˆ«æ˜ å°„
        self.relation_label_to_idx = {rel: idx for idx, rel in enumerate(self.rel_categories)}
        self.idx_to_relation_label = {idx: rel for idx, rel in enumerate(self.rel_categories)}
        
        # 3. åŠ è½½å¯¹è±¡ç±»åˆ«ï¼ˆæ— èƒŒæ™¯ç±»åˆ«ï¼‰
        self.obj_categories = [
            'arrow',
            'connector',
            'crossing',
            'general',
            'instrumentation',
            'valve',
            # 'background' è¢«è¿‡æ»¤
        ]
        
        self.label_to_idx = {
            obj: idx for idx, obj in enumerate(self.obj_categories)
        }
        
        self.idx_to_label = {
            idx: obj for idx, obj in enumerate(self.obj_categories)
        }
        
        # æ£€æŸ¥æ•°æ®ç›®å½•æ˜¯å¦å­˜åœ¨
        if not os.path.exists(self.base_dir):
            print(f"âš ï¸ è­¦å‘Š: PIDæ•°æ®ç›®å½•ä¸å­˜åœ¨ {self.base_dir}")
            print(f"   è¯·ç¡®ä¿æ•°æ®è·¯å¾„æ­£ç¡®ï¼Œæˆ–æ•°æ®å·²æ­£ç¡®æ”¾ç½®")
            self.valid_samples = []
            self._create_minimal_mappings()
            return
            
        # æå–æ•°æ®é›†ä¿¡æ¯
        self.extractor = PIDGraphDatasetExtractor(self.base_dir)
        self.paired_files = self.extractor.extract()
        
        # éªŒè¯å¹¶è¿‡æ»¤æœ‰æ•ˆæ ·æœ¬
        self.valid_samples = []
        print("éªŒè¯æ ·æœ¬...")
        
        for i, pair in enumerate(self.paired_files):
            annotations = self.extractor.extract_annotations_from_graphml(pair['annotation_path'])
            
            if annotations and annotations['num_objects'] >= min_objects:
                pair['annotations'] = annotations
                self.valid_samples.append(pair)
        
        print(f"\næ•°æ®é›†åŠ è½½å®Œæˆ:")
        print(f"  åŸå§‹é…å¯¹: {len(self.paired_files)}")
        print(f"  æœ‰æ•ˆæ ·æœ¬: {len(self.valid_samples)}")
        
        if len(self.valid_samples) > 0:
            self._create_label_mappings()
        else:
            self._create_minimal_mappings()

    def _create_minimal_mappings(self):
        """åˆ›å»ºæœ€å°æ ‡ç­¾æ˜ å°„ï¼ˆå½“æ²¡æœ‰æœ‰æ•ˆæ ·æœ¬æ—¶ï¼‰"""
        self.object_classes = self.obj_categories
        self.relation_classes = self.rel_categories
        self.object_label_to_idx = self.label_to_idx
        self.idx_to_object_label = self.idx_to_label
        self.num_object_classes = len(self.object_classes)
        self.num_relation_classes = len(self.relation_classes)
        print(f"  ä½¿ç”¨é»˜è®¤ç±»åˆ«æ˜ å°„")

    def _create_label_mappings(self):
        """åˆ›å»ºæ ‡ç­¾æ˜ å°„"""
        all_object_labels = set()
        all_relation_labels = set()
        
        for sample in self.valid_samples:
            for obj in sample['annotations']['objects']:
                all_object_labels.add(obj['label'])
            for rel in sample['annotations']['relations']:
                all_relation_labels.add(rel['predicate'])
        
        # å¯¹è±¡æ ‡ç­¾æ˜ å°„ï¼ˆæ— èƒŒæ™¯ç±»ï¼‰
        self.object_classes = sorted(list(all_object_labels))
        self.object_label_to_idx = {label: idx for idx, label in enumerate(self.object_classes)}
        self.idx_to_object_label = {idx: label for label, idx in self.object_label_to_idx.items()}
        self.num_object_classes = len(self.object_classes)
        
        # å…³ç³»æ ‡ç­¾æ˜ å°„ï¼ˆæ— èƒŒæ™¯ç±»ï¼‰
        self.relation_classes = sorted(list(all_relation_labels))
        self.relation_label_to_idx = {label: idx for idx, label in enumerate(self.relation_classes)}
        self.idx_to_relation_label = {idx: label for label, idx in self.relation_label_to_idx.items()}
        self.num_relation_classes = len(self.relation_classes)
        
        print(f"  å¯¹è±¡ç±»åˆ«: {self.num_object_classes} ä¸ª")
        print(f"  å…³ç³»ç±»åˆ«: {self.num_relation_classes} ä¸ª")

    def __len__(self):
        return len(self.valid_samples)

    def __getitem__(self, idx):
        """è¿”å›å›¾åƒã€ç›®æ ‡æ ‡æ³¨å’Œæ ·æœ¬ä¿¡æ¯"""
        if len(self.valid_samples) == 0:
            # è¿”å›ç©ºæ ·æœ¬
            dummy_image = Image.new('RGB', (800, 600), color=(128, 128, 128))
            if self.transform:
                dummy_image = self.transform(dummy_image)
            
            target = {
                'boxes': torch.zeros((0, 4), dtype=torch.float32),
                'labels': torch.zeros((0,), dtype=torch.int64),
                'image_id': torch.tensor([idx + 1], dtype=torch.int64),
                'orig_size': torch.tensor([600, 800]),
                'rel_annotations': torch.zeros((0, 3), dtype=torch.int64),
                'size': torch.tensor([600, 800]),
                'iscrowd': torch.zeros((0,), dtype=torch.int64)
            }
            return dummy_image, target
            
        sample = self.valid_samples[idx]
        
        # 1. å®‰å…¨åŠ è½½å›¾åƒ
        try:
            image = Image.open(sample['image_path']).convert('RGB')
        except Exception as e:
            print(f"âš ï¸ è­¦å‘Š: æ— æ³•åŠ è½½å›¾åƒ {sample['image_path']}: {e}")
            image = Image.new('RGB', (800, 600), color=(128, 128, 128))
            
        orig_size = torch.tensor([image.size[1], image.size[0]])  # (height, width)
        
        # 2. å¤„ç†æ ‡æ³¨
        annotations = sample['annotations']
        
        # æå–å¯¹è±¡ä¿¡æ¯
        boxes = []
        labels = []
        object_ids = []
        
        for obj in annotations['objects']:
            boxes.append(obj['bbox'])  # [xmin, ymin, xmax, ymax]
            # è·å–æ ‡ç­¾ç´¢å¼•ï¼ˆè·³è¿‡èƒŒæ™¯ç±»ï¼Œä»1å¼€å§‹ï¼‰
            label_idx = self.object_label_to_idx.get(obj['label'], 1)  # é»˜è®¤ä¸ºç¬¬ä¸€ä¸ªçœŸå®ç±»åˆ«
            labels.append(label_idx)
            object_ids.append(obj['id'])
        
        # æå–å…³ç³»ä¿¡æ¯
        rel_annotations = []
        for rel in annotations['relations']:
            try:
                # æ‰¾åˆ°ä¸»ä½“å’Œå®¢ä½“åœ¨å¯¹è±¡åˆ—è¡¨ä¸­çš„ç´¢å¼•
                sub_idx = object_ids.index(rel['subject'])
                obj_idx = object_ids.index(rel['object'])
                rel_label = self.relation_label_to_idx.get(rel['predicate'], 1)
                rel_annotations.append([sub_idx, obj_idx, rel_label])
            except ValueError:
                # è·³è¿‡æ‰¾ä¸åˆ°å¯¹åº”å¯¹è±¡çš„å…³ç³»
                continue
        
        # 3. è½¬æ¢ä¸ºå¼ é‡
        if len(boxes) > 0:
            boxes = torch.as_tensor(boxes, dtype=torch.float32)
            labels = torch.as_tensor(labels, dtype=torch.int64)
        else:
            # å¤„ç†æ²¡æœ‰å¯¹è±¡çš„æƒ…å†µ
            boxes = torch.zeros((0, 4), dtype=torch.float32)
            labels = torch.zeros((0,), dtype=torch.int64)
        
        if len(rel_annotations) > 0:
            rel_annotations = torch.as_tensor(rel_annotations, dtype=torch.int64)
        else:
            rel_annotations = torch.zeros((0, 3), dtype=torch.int64)
        
        # 4. æ„å»ºç›®æ ‡å­—å…¸
        target = {
            'boxes': boxes,                    # [N, 4] è¾¹ç•Œæ¡†
            'labels': labels,                  # [N] å¯¹è±¡æ ‡ç­¾
            'image_id': torch.tensor([idx + 1], dtype=torch.int64),   # [1] å›¾åƒID
            'orig_size': orig_size,            # [2] åŸå§‹å›¾åƒå°ºå¯¸ [H, W]
            'rel_annotations': rel_annotations,  # [M, 3] - [subject_idx, object_idx, relation_label]
            'size': orig_size,              # [2] å›¾åƒå°ºå¯¸ [H, W]
            'iscrowd': torch.zeros(len(boxes), dtype=torch.int64)
        }
        
        # 5. åº”ç”¨å›¾åƒå˜æ¢
        if self.transform:
            image, target = self.transform(image, target)
        
        return image, target

    def get_dataset_info(self):
        """è·å–æ•°æ®é›†ä¿¡æ¯"""
        return {
            'total_samples': len(self.valid_samples),
            'object_classes': self.object_classes,
            'relation_classes': self.relation_classes,
            'num_object_classes': self.num_object_classes,
            'num_relation_classes': self.num_relation_classes
        }


def create_datasets(complete_img=False, batch_size=2, split_ratio=0.8, base_path='data/PID', max_samples: int = None):
    """Create PID train/val datasets compatible with main.py usage.
    Returns (train_dataset, val_dataset).
    """
    # basic transforms: ToTensor + Normalize + box normalization inside Normalize
    transform = Compose([
        ToTensor(),
        Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])
    ])
    ds = PIDGraphDataset(complete_img=complete_img, transform=transform, base_path=base_path)
    n = len(ds)
    if n == 0:
        print("âš ï¸ PIDGraphDataset has 0 samples; returning empty split.")
        return ds, ds
    import math
    indices = list(range(n))
    # Limit total samples for quick runs if requested
    if isinstance(max_samples, int) and max_samples > 0:
        max_samples = min(max_samples, n)
        indices = indices[:max_samples]
        n = len(indices)
    n_train = int(math.floor(n * split_ratio))
    train_indices = indices[:n_train]
    val_indices = indices[n_train:]
    from torch.utils.data import Subset
    return Subset(ds, train_indices), Subset(ds, val_indices)


class CocoCompatibleDataset(torchvision.datasets.CocoDetection):
    """COCO å…¼å®¹çš„æ•°æ®é›†åŒ…è£…ç±»"""
    
    def __init__(self, original_dataset, indices=None):
        # ä¸è°ƒç”¨çˆ¶ç±»çš„ __init__ï¼Œå› ä¸ºæˆ‘ä»¬è¦è‡ªå®šä¹‰è¡Œä¸º
        self.original_dataset = original_dataset
        self.indices = indices if indices is not None else list(range(len(original_dataset)))
        
        # ğŸ”‘ åˆ›å»º COCO API å¯¹è±¡
        self.coco = self._create_coco_api()
        
        # å¤åˆ¶åŸæ•°æ®é›†çš„å±æ€§
        self.rel_categories = getattr(original_dataset, 'rel_categories', ['__background__'])
        self.obj_categories = getattr(original_dataset, 'obj_categories', ['__background__'])
        
    def __len__(self):
        return len(self.indices)
    
    def __getitem__(self, idx):
        # è·å–çœŸå®çš„ç´¢å¼•
        real_idx = self.indices[idx]
        return self.original_dataset[real_idx]
    
    def _create_coco_api(self):
        """åˆ›å»º COCO API å¯¹è±¡"""
        try:
            # æ„å»º COCO æ ¼å¼æ•°æ®
            images = []
            annotations = []
            categories = []
            
            # è·å–ç±»åˆ«ä¿¡æ¯
            obj_categories = getattr(self.original_dataset, 'obj_categories', ['__background__', 'object'])
            
            # åˆ›å»ºç±»åˆ«ï¼ˆè·³è¿‡èƒŒæ™¯ç±»ï¼‰
            for i, cat_name in enumerate(obj_categories):
                if i == 0:  # è·³è¿‡èƒŒæ™¯ç±»
                    continue
                categories.append({
                    "id": i,
                    "name": cat_name,
                    "supercategory": "thing"
                })
            
            if not categories:
                categories.append({"id": 1, "name": "object", "supercategory": "thing"})
            
            # å¤„ç†æ ·æœ¬ï¼ˆé™åˆ¶æ•°é‡é¿å…å†…å­˜é—®é¢˜ï¼‰
            num_samples = min(len(self.indices), 50)
            ann_id = 1
            
            for i in range(num_samples):
                try:
                    real_idx = self.indices[i]
                    image, target = self.original_dataset[real_idx]
                    
                    # å›¾åƒä¿¡æ¯
                    if 'orig_size' in target:
                        h, w = target['orig_size'].tolist()
                    else:
                        h, w = 800, 800
                    
                    image_info = {
                        "id": int(real_idx) + 1,
                        "width": w,
                        "height": h,
                        "file_name": f"image_{real_idx}.jpg"
                    }
                    images.append(image_info)
                    
                    # æ ‡æ³¨ä¿¡æ¯
                    if 'boxes' in target and 'labels' in target:
                        boxes = target['boxes']
                        labels = target['labels']
                        
                        for box, label in zip(boxes, labels):
                            if len(box) == 4 and label.item() > 0:
                                x1, y1, x2, y2 = box.tolist()
                                annotation = {
                                    "id": ann_id,
                                    "image_id": int(real_idx) + 1,
                                    "category_id": int(label.item()),
                                    "bbox": [x1, y1, x2 - x1, y2 - y1],
                                    "area": (x2 - x1) * (y2 - y1),
                                    "iscrowd": 0
                                }
                                annotations.append(annotation)
                                ann_id += 1
                except Exception as e:
                    print(f"å¤„ç†æ ·æœ¬ {i} æ—¶å‡ºé”™: {e}")
                    continue
            
            # ç¡®ä¿è‡³å°‘æœ‰ä¸€ä¸ªå›¾åƒå’Œæ ‡æ³¨
            if not images:
                images.append({"id": 1, "width": 800, "height": 800, "file_name": "dummy.jpg"})
            
            if not annotations:
                annotations.append({
                    "id": 1, "image_id": 1, "category_id": 1,
                    "bbox": [100, 100, 100, 100], "area": 10000, "iscrowd": 0
                })
            
            # åˆ›å»º COCO æ•°æ®
            coco_data = {
                "images": images,
                "annotations": annotations,
                "categories": categories,
                "info": {"description": "PID Dataset in COCO format", "version": "1.0"}
            }
            
            # åˆ›å»ºä¸´æ—¶æ–‡ä»¶
            temp_file = tempfile.NamedTemporaryFile(mode='w', suffix='.json', delete=False)
            json.dump(coco_data, temp_file, indent=2)
            temp_file.close()
            
            try:
                coco_api = COCO(temp_file.name)
                os.unlink(temp_file.name)
                print(f"âœ… åˆ›å»º COCO API: {len(images)} å›¾åƒ, {len(annotations)} æ ‡æ³¨")
                return coco_api
            except Exception as e:
                print(f"âŒ COCO API åˆ›å»ºå¤±è´¥: {e}")
                if os.path.exists(temp_file.name):
                    os.unlink(temp_file.name)
                return self._create_minimal_coco_api()
                
        except Exception as e:
            print(f"âŒ _create_coco_api å‡ºé”™: {e}")
            return self._create_minimal_coco_api()
    
    def _create_minimal_coco_api(self):
        """åˆ›å»ºæœ€å°çš„ COCO API"""
        dummy_data = {
            "images": [{"id": 1, "width": 800, "height": 800, "file_name": "dummy.jpg"}],
            "annotations": [{"id": 1, "image_id": 1, "category_id": 1, "bbox": [100, 100, 100, 100], "area": 10000, "iscrowd": 0}],
            "categories": [{"id": 1, "name": "object", "supercategory": "thing"}]
        }
        
        temp_file = tempfile.NamedTemporaryFile(mode='w', suffix='.json', delete=False)
        json.dump(dummy_data, temp_file)
        temp_file.close()
        
        try:
            coco_api = COCO(temp_file.name)
            os.unlink(temp_file.name)
            return coco_api
        except:
            if os.path.exists(temp_file.name):
                os.unlink(temp_file.name)
            return None


def create_transforms(train=True):
    """åˆ›å»ºæ•°æ®å˜æ¢ - ä½¿ç”¨å›ºå®šå°ºå¯¸ç¡®ä¿è¾¹ç•Œæ¡†æ­£ç¡®å½’ä¸€åŒ–"""
    import torch
    from .transforms import Compose, ToTensor, Normalize
    
    # åˆ›å»ºä¸€ä¸ªè‡ªå®šä¹‰çš„å›ºå®šå°ºå¯¸resize
    class FixedResize(object):
        def __init__(self, size):
            self.size = size  # (width, height)
        
        def __call__(self, image, target=None):
            from torchvision.transforms import functional as F
            w, h = image.size
            new_w, new_h = self.size
            
            # resizeå›¾åƒ
            resized_image = F.resize(image, (new_h, new_w))
            
            if target is None:
                return resized_image, None
                
            # è®¡ç®—ç¼©æ”¾æ¯”ä¾‹
            ratio_w = new_w / w
            ratio_h = new_h / h
            
            target = target.copy()
            if "boxes" in target:
                boxes = target["boxes"]
                # ç¼©æ”¾è¾¹ç•Œæ¡†åæ ‡
                scaled_boxes = boxes * torch.as_tensor([ratio_w, ratio_h, ratio_w, ratio_h])
                target["boxes"] = scaled_boxes
                
            # æ›´æ–°å°ºå¯¸ä¿¡æ¯
            if "size" in target:
                target["size"] = torch.tensor([new_h, new_w])
            if "orig_size" in target:
                target["orig_size"] = torch.tensor([new_h, new_w])
                
            return resized_image, target
    
    normalize = Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])
    
    return Compose([
        FixedResize((1024, 1024)),  # å›ºå®šå°ºå¯¸1024x1024
        ToTensor(),
        normalize,
    ])


def build_pid_dataset(image_set, args):
    """æ„å»ºPIDæ•°æ®é›†"""
    # è·å–æ•°æ®è·¯å¾„
    if hasattr(args, 'pid_path'):
        base_path = args.pid_path
    else:
        base_path = 'data/PID'
    
    # åˆ›å»ºå˜æ¢
    transform = create_transforms(train=(image_set == 'train'))
    
    # åˆ›å»ºåŸå§‹æ•°æ®é›†
    dataset = PIDGraphDataset(
        complete_img=True,   # ä½¿ç”¨è°ƒæ•´åçš„å®Œæ•´å›¾åƒæ•°æ®
        min_objects=1,
        transform=transform,
        base_path=base_path
    )
    
    # å¦‚æœæ²¡æœ‰æœ‰æ•ˆæ ·æœ¬ï¼Œç›´æ¥è¿”å›åŸæ•°æ®é›†
    if len(dataset) == 0:
        print(f"âš ï¸ è­¦å‘Š: {image_set} æ•°æ®é›†ä¸ºç©º")
        return dataset
    
    # åˆ’åˆ†è®­ç»ƒé›†å’ŒéªŒè¯é›†
    total_size = len(dataset)
    if image_set == 'train':
        # ä½¿ç”¨å‰80%ä½œä¸ºè®­ç»ƒé›†
        train_size = int(total_size * 0.8)
        indices = list(range(train_size))
    else:
        # ä½¿ç”¨å20%ä½œä¸ºéªŒè¯é›†
        train_size = int(total_size * 0.8)
        indices = list(range(train_size, total_size))
    
    # åˆ›å»ºCOCOå…¼å®¹æ•°æ®é›†
    coco_dataset = CocoCompatibleDataset(dataset, indices)
    
    print(f"âœ… æ„å»º {image_set} æ•°æ®é›†: {len(coco_dataset)} æ ·æœ¬")
    
    return coco_dataset
