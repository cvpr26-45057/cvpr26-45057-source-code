#!/usr/bin/env python3
"""
æ”¹è¿›çš„è®­ç»ƒé…ç½® - è§£å†³æ”¶æ•›æ…¢çš„é—®é¢˜
åŸºäºæ”¶æ•›åˆ†æç»“æœçš„ä¼˜åŒ–æ–¹æ¡ˆ
"""

import torch
import torch.optim as optim
from torch.optim.lr_scheduler import CosineAnnealingLR, MultiStepLR, ReduceLROnPlateau

def get_improved_optimizer_and_scheduler(model, args):
    """
    è·å–æ”¹è¿›çš„ä¼˜åŒ–å™¨å’Œå­¦ä¹ ç‡è°ƒåº¦å™¨
    """
    
    # 1. åˆ†å±‚å­¦ä¹ ç‡è®¾ç½®
    backbone_params = []
    transformer_params = []
    head_params = []
    
    for name, param in model.named_parameters():
        if 'backbone' in name:
            backbone_params.append(param)
        elif 'transformer' in name or 'encoder' in name or 'decoder' in name:
            transformer_params.append(param)
        else:
            head_params.append(param)
    
    # 2. æ”¹è¿›çš„ä¼˜åŒ–å™¨é…ç½®
    optimizer = optim.AdamW([
        {'params': backbone_params, 'lr': args.lr * 0.1},  # backboneç”¨æ›´å°å­¦ä¹ ç‡
        {'params': transformer_params, 'lr': args.lr},      # transformeræ­£å¸¸å­¦ä¹ ç‡
        {'params': head_params, 'lr': args.lr * 2.0}       # æ£€æµ‹å¤´ç”¨æ›´å¤§å­¦ä¹ ç‡
    ], 
    lr=args.lr, 
    weight_decay=args.weight_decay,
    betas=(0.9, 0.999),
    eps=1e-8
    )
    
    # 3. æ”¹è¿›çš„å­¦ä¹ ç‡è°ƒåº¦ç­–ç•¥
    if hasattr(args, 'lr_scheduler') and args.lr_scheduler == 'cosine':
        # ä½™å¼¦é€€ç«è°ƒåº¦
        scheduler = CosineAnnealingLR(
            optimizer, 
            T_max=args.epochs,
            eta_min=args.lr * 0.01  # æœ€å°å­¦ä¹ ç‡ä¸ºåˆå§‹çš„1%
        )
    elif hasattr(args, 'lr_scheduler') and args.lr_scheduler == 'plateau':
        # è‡ªé€‚åº”è°ƒåº¦ï¼ˆæ¨èï¼‰
        scheduler = ReduceLROnPlateau(
            optimizer,
            mode='min',
            factor=0.5,        # å­¦ä¹ ç‡å‡åŠ
            patience=5,        # 5ä¸ªepochæ²¡æ”¹å–„å°±é™ä½
            min_lr=args.lr * 0.001,  # æœ€å°å­¦ä¹ ç‡
            verbose=True
        )
    else:
        # å¤šæ­¥è°ƒåº¦ï¼ˆåŸæ–¹æ¡ˆæ”¹è¿›ï¼‰
        milestones = [args.lr_drop, args.lr_drop + 10, args.lr_drop + 20]
        scheduler = MultiStepLR(
            optimizer,
            milestones=milestones,
            gamma=0.5  # æ¯æ¬¡é™ä½50%
        )
    
    return optimizer, scheduler

def get_improved_training_config():
    """
    è·å–æ”¹è¿›çš„è®­ç»ƒé…ç½®
    """
    config = {
        # å­¦ä¹ ç‡ç­–ç•¥
        'initial_lr': 5e-5,  # é™ä½åˆå§‹å­¦ä¹ ç‡
        'lr_scheduler': 'plateau',  # ä½¿ç”¨è‡ªé€‚åº”è°ƒåº¦
        'warmup_epochs': 3,  # æ·»åŠ warmup
        
        # æ•°æ®å¢å¼º
        'enhanced_augmentation': True,
        'mixup_alpha': 0.2,  # æ·»åŠ mixup
        'cutmix_alpha': 0.2,  # æ·»åŠ cutmix
        
        # æ­£åˆ™åŒ–
        'weight_decay': 1e-4,
        'dropout': 0.1,
        'label_smoothing': 0.1,
        
        # æ¢¯åº¦ä¼˜åŒ–
        'gradient_accumulation_steps': 4,  # ç´¯ç§¯4æ­¥ç›¸å½“äºbatch_size=8
        'max_grad_norm': 0.1,  # æ¢¯åº¦è£å‰ª
        
        # æ—©åœç­–ç•¥
        'early_stopping_patience': 15,
        'save_best_only': True,
        
        # æŸ¥è¯¢æ•°é‡åŠ¨æ€è°ƒæ•´
        'dynamic_queries': True,
        'start_queries': (400, 800),  # å¼€å§‹æ—¶ç”¨è¾ƒå°‘æŸ¥è¯¢
        'end_queries': (800, 1600),   # åæœŸå¢åŠ åˆ°å®Œæ•´æŸ¥è¯¢
        'query_ramp_epochs': 20,
    }
    
    return config

def apply_warmup_schedule(optimizer, epoch, warmup_epochs, base_lr):
    """
    åº”ç”¨å­¦ä¹ ç‡warmup
    """
    if epoch < warmup_epochs:
        warmup_factor = (epoch + 1) / warmup_epochs
        for param_group in optimizer.param_groups:
            param_group['lr'] = base_lr * warmup_factor

def get_dynamic_queries(epoch, config):
    """
    åŠ¨æ€è°ƒæ•´æŸ¥è¯¢æ•°é‡
    """
    if not config.get('dynamic_queries', False):
        return config.get('end_queries', (800, 1600))
    
    start_obj, start_rel = config['start_queries']
    end_obj, end_rel = config['end_queries']
    ramp_epochs = config['query_ramp_epochs']
    
    if epoch < ramp_epochs:
        progress = epoch / ramp_epochs
        current_obj = int(start_obj + (end_obj - start_obj) * progress)
        current_rel = int(start_rel + (end_rel - start_rel) * progress)
    else:
        current_obj, current_rel = end_obj, end_rel
    
    return current_obj, current_rel

if __name__ == "__main__":
    print("ğŸš€ æ”¹è¿›çš„è®­ç»ƒé…ç½®")
    print("="*50)
    
    config = get_improved_training_config()
    
    print("ğŸ“‹ ä¸»è¦æ”¹è¿›ç‚¹:")
    print(f"1. é™ä½åˆå§‹å­¦ä¹ ç‡: {config['initial_lr']}")
    print(f"2. ä½¿ç”¨è‡ªé€‚åº”è°ƒåº¦: {config['lr_scheduler']}")
    print(f"3. æ¢¯åº¦ç´¯ç§¯æ­¥æ•°: {config['gradient_accumulation_steps']}")
    print(f"4. æ—©åœè€å¿ƒå€¼: {config['early_stopping_patience']}")
    print(f"5. åŠ¨æ€æŸ¥è¯¢è°ƒæ•´: {config['dynamic_queries']}")
    
    print("\nğŸ¯ é¢„æœŸæ•ˆæœ:")
    print("- æ›´ç¨³å®šçš„æ”¶æ•›è¿‡ç¨‹")
    print("- é¿å…åæœŸéœ‡è¡")
    print("- æ›´å¥½çš„æ³›åŒ–æ€§èƒ½")
    print("- è‡ªåŠ¨å­¦ä¹ ç‡è°ƒæ•´")
