#!/usr/bin/env python3
"""
PID æ¨ç†è„šæœ¬ - ä½¿ç”¨è®­ç»ƒå¥½çš„ RelTRv3 æ¨¡å‹è¿›è¡Œåœºæ™¯å›¾ç”Ÿæˆ
"""

import argparse
import torch
import numpy as np
from PIL import Image, ImageDraw, ImageFont
import os
import json
from pathlib import Path
import matplotlib.pyplot as plt
import matplotlib.patches as patches

# å¯¼å…¥æ¨¡å‹å’Œæ•°æ®é›†ç›¸å…³æ¨¡å—
import sys
sys.path.append('.')

# build_reltr will be imported lazily in load_model to avoid heavy module imports at import time
from datasets.pid_datasets import build_pid_dataset
from datasets.transforms import Compose, Normalize, ToTensor
from util.misc import nested_tensor_from_tensor_list
import util.misc as utils

# PIDæ•°æ®é›†çš„ç±»åˆ«æ˜ å°„ (ç®€åŒ–ç‰ˆæœ¬ï¼Œä¸è®­ç»ƒæ—¶åŒ¹é…)
PID_CLASSES = [
    'background', 'person', 'bicycle', 'chair', 'table', 'handbag', 'laptop'
]

PID_PREDICATES = [
    'background', 'hold', 'sit_on'
]

def get_args_parser():
    parser = argparse.ArgumentParser('PID Inference', add_help=False)
    
    # Model parameters
    parser.add_argument('--backbone', default='resnet50', type=str,
                        help="Name of the convolutional backbone to use")
    parser.add_argument('--position_embedding', default='sine', type=str, choices=('sine', 'learned'),
                        help="Type of positional embedding to use on top of the image features")
    parser.add_argument('--dilation', action='store_true',
                        help="If true, we replace stride with dilation in the last convolutional block (DC5)")
    parser.add_argument('--return_interm_layers', action='store_true',
                        help="Return the fpn if there is the tag")
    
    # PID dataset parameters (adjusted to match actual training)
    parser.add_argument('--num_classes', default=7, type=int,
                        help="Number of object classes (excluding background)")
    parser.add_argument('--num_rel_classes', default=2, type=int,
                        help="Number of relation classes (excluding background)")
    parser.add_argument('--num_entities', default=800, type=int,
                        help="Number of entity queries") 
    parser.add_argument('--num_triplets', default=800, type=int,
                        help="Number of triplet queries")
    
    # Transformer parameters
    parser.add_argument('--hidden_dim', default=256, type=int,
                        help="Size of the embeddings (dimension of the transformer)")
    parser.add_argument('--dropout', default=0.1, type=float,
                        help="Dropout applied in the transformer")
    parser.add_argument('--nheads', default=8, type=int,
                        help="Number of attention heads inside the transformer's attentions")
    parser.add_argument('--dim_feedforward', default=2048, type=int,
                        help="Intermediate size of the feedforward layers in the transformer blocks")
    parser.add_argument('--enc_layers', default=6, type=int,
                        help="Number of encoding layers in the transformer")
    parser.add_argument('--dec_layers', default=6, type=int,
                        help="Number of decoding layers in the transformer")
    parser.add_argument('--pre_norm', action='store_true')
    
    # Matcher parameters
    parser.add_argument('--set_cost_class', default=1, type=float,
                        help="Class coefficient in the matching cost")
    parser.add_argument('--set_cost_bbox', default=5, type=float,
                        help="L1 box coefficient in the matching cost")
    parser.add_argument('--set_cost_giou', default=2, type=float,
                        help="giou box coefficient in the matching cost")
    parser.add_argument('--set_iou_threshold', default=0.7, type=float,
                        help="IoU threshold for matching")
    
    # Loss coefficients (needed for model building)
    parser.add_argument('--mask_loss_coef', default=1, type=float)
    parser.add_argument('--dice_loss_coef', default=1, type=float)
    parser.add_argument('--bbox_loss_coef', default=5, type=float)
    parser.add_argument('--giou_loss_coef', default=2, type=float)
    parser.add_argument('--rel_loss_coef', default=1, type=float)
    parser.add_argument('--eos_coef', default=0.1, type=float,
                        help="Relative classification weight of the no-object class")
    
    # Additional model parameters for building
    parser.add_argument('--lr_backbone', default=1e-5, type=float,
                        help="Learning rate for backbone (used in model building)")
    parser.add_argument('--masks', action='store_true',
                        help="Train segmentation head if the flag is provided")
    parser.add_argument('--aux_loss', default=True, type=bool,
                        help="auxiliary decoding losses (loss at each layer)")
    
    # Dataset parameters  
    parser.add_argument('--dataset_file', default='pid')
    parser.add_argument('--data_path', type=str, default='data/pid_resized/')
    parser.add_argument('--remove_difficult', action='store_true')
    
    # Inference parameters
    parser.add_argument('--model_path', required=True, type=str,
                        help="Path to the trained model checkpoint")
    parser.add_argument('--image_path', required=True, type=str,
                        help="Path to the input image")
    parser.add_argument('--output_dir', default='inference_results', type=str,
                        help="Output directory for results")
    parser.add_argument('--confidence_threshold', default=0.5, type=float,
                        help="Confidence threshold for predictions")
    parser.add_argument('--max_detections', default=100, type=int,
                        help="Maximum number of detections")
    parser.add_argument('--device', default='cuda', type=str,
                        help="Device to use (cuda or cpu)")
    
    return parser

def load_model(args):
    """åŠ è½½è®­ç»ƒå¥½çš„æ¨¡å‹"""
    print(f"ğŸ”„ åŠ è½½æ¨¡å‹ä»: {args.model_path}")
    
    device = torch.device(args.device)
    
    # æ„å»ºæ¨¡å‹ï¼ˆå»¶è¿Ÿå¯¼å…¥ä»¥é¿å…åœ¨è„šæœ¬å¯¼å…¥æ—¶è§¦å‘å¤§é‡ä¾èµ–ï¼‰
    from importlib import import_module
    build_fn = import_module('models.reltr').build_reltr
    model, _, _ = build_fn(args)
    model.to(device)
    
    # åŠ è½½æ£€æŸ¥ç‚¹
    checkpoint = torch.load(args.model_path, map_location='cpu')
    model.load_state_dict(checkpoint['model'])
    model.eval()
    
    print(f"âœ… æ¨¡å‹åŠ è½½å®Œæˆï¼Œå‚æ•°æ•°é‡: {sum(p.numel() for p in model.parameters()):,}")
    return model, device

def preprocess_image(image_path):
    """é¢„å¤„ç†è¾“å…¥å›¾åƒ"""
    print(f"ğŸ”„ é¢„å¤„ç†å›¾åƒ: {image_path}")
    
    # åŠ è½½å›¾åƒ
    image = Image.open(image_path).convert('RGB')
    orig_size = torch.tensor([image.height, image.width])
    
    # åº”ç”¨å˜æ¢
    normalize = Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])
    transform = Compose([
        ToTensor(),
        normalize,
    ])
    transformed_image, _ = transform(image, None)
    
    # åˆ›å»ºnested tensor
    samples = nested_tensor_from_tensor_list([transformed_image])
    
    print(f"âœ… å›¾åƒé¢„å¤„ç†å®Œæˆï¼ŒåŸå§‹å°ºå¯¸: {image.size}, å˜æ¢åå°ºå¯¸: {transformed_image.shape}")
    return samples, orig_size, image

def postprocess_predictions(outputs, orig_size, confidence_threshold=0.5, max_detections=100):
    """åå¤„ç†æ¨¡å‹é¢„æµ‹ç»“æœ"""
    print(f"ğŸ”„ åå¤„ç†é¢„æµ‹ç»“æœï¼Œç½®ä¿¡åº¦é˜ˆå€¼: {confidence_threshold}")
    
    # è·å–é¢„æµ‹ç»“æœ
    pred_logits = outputs['pred_logits'][0]  # [num_queries, num_classes]
    pred_boxes = outputs['pred_boxes'][0]    # [num_queries, 4]
    sub_logits = outputs['sub_logits'][0]    # [num_queries, num_classes] 
    obj_logits = outputs['obj_logits'][0]    # [num_queries, num_classes]
    rel_logits = outputs['rel_logits'][0]    # [num_queries, num_predicates]
    
    # è·å–è¾¹ç•Œæ¡†é¢„æµ‹
    pred_scores = pred_logits.softmax(-1)[:, :-1]  # æ’é™¤èƒŒæ™¯ç±»
    pred_labels = pred_scores.argmax(-1)
    pred_scores = pred_scores.max(-1)[0]
    
    # è¿‡æ»¤ä½ç½®ä¿¡åº¦æ£€æµ‹
    keep = pred_scores > confidence_threshold
    if keep.sum() == 0:
        print("âš ï¸ æ²¡æœ‰æ‰¾åˆ°ç½®ä¿¡åº¦è¶³å¤Ÿé«˜çš„æ£€æµ‹ç»“æœ")
        return [], []
    
    # é™åˆ¶æ£€æµ‹æ•°é‡
    if keep.sum() > max_detections:
        top_scores, top_indices = pred_scores[keep].topk(max_detections)
        keep_indices = torch.where(keep)[0][top_indices]
        keep = torch.zeros_like(keep, dtype=torch.bool)
        keep[keep_indices] = True
    
    # æå–æœ‰æ•ˆæ£€æµ‹
    valid_boxes = pred_boxes[keep]
    valid_labels = pred_labels[keep]
    valid_scores = pred_scores[keep]
    
    # è½¬æ¢è¾¹ç•Œæ¡†åæ ‡åˆ°åŸå§‹å›¾åƒå°ºå¯¸
    # boxes æ ¼å¼: [cx, cy, w, h] (å½’ä¸€åŒ–) -> [x1, y1, x2, y2] (åƒç´ )
    h, w = orig_size
    valid_boxes[:, [0, 2]] *= w  # cx, w
    valid_boxes[:, [1, 3]] *= h  # cy, h
    
    # è½¬æ¢ä¸º x1, y1, x2, y2 æ ¼å¼
    boxes_x1y1x2y2 = torch.zeros_like(valid_boxes)
    boxes_x1y1x2y2[:, 0] = valid_boxes[:, 0] - valid_boxes[:, 2] / 2  # x1
    boxes_x1y1x2y2[:, 1] = valid_boxes[:, 1] - valid_boxes[:, 3] / 2  # y1
    boxes_x1y1x2y2[:, 2] = valid_boxes[:, 0] + valid_boxes[:, 2] / 2  # x2
    boxes_x1y1x2y2[:, 3] = valid_boxes[:, 1] + valid_boxes[:, 3] / 2  # y2
    
    # å¤„ç†å…³ç³»é¢„æµ‹
    sub_scores = sub_logits[keep].softmax(-1)[:, :-1]
    obj_scores = obj_logits[keep].softmax(-1)[:, :-1] 
    rel_scores = rel_logits[keep].softmax(-1)[:, :-1]  # æ’é™¤èƒŒæ™¯å…³ç³»
    
    # æ„å»ºæ£€æµ‹ç»“æœ
    detections = []
    for i in range(len(valid_boxes)):
        detection = {
            'bbox': boxes_x1y1x2y2[i].cpu().numpy(),
            'label': valid_labels[i].item(),
            'class_name': PID_CLASSES[valid_labels[i].item()],
            'confidence': valid_scores[i].item()
        }
        detections.append(detection)
    
    # æ„å»ºå…³ç³»ç»“æœï¼ˆç®€åŒ–ç‰ˆæœ¬ï¼Œé€‰æ‹©æœ€é«˜ç½®ä¿¡åº¦çš„å…³ç³»ï¼‰
    relations = []
    for i in range(len(valid_boxes)):
        for j in range(len(valid_boxes)):
            if i != j:
                # ä½¿ç”¨å¯¹åº”çš„å…³ç³»åˆ†æ•°
                rel_score = rel_scores[i].max().item()
                rel_pred = rel_scores[i].argmax().item()
                
                if rel_score > confidence_threshold:
                    relation = {
                        'subject': i,
                        'object': j,
                        'predicate': rel_pred,
                        'predicate_name': PID_PREDICATES[rel_pred] if rel_pred < len(PID_PREDICATES) else f'rel_{rel_pred}',
                        'confidence': rel_score
                    }
                    relations.append(relation)
    
    print(f"âœ… åå¤„ç†å®Œæˆï¼Œæ£€æµ‹åˆ° {len(detections)} ä¸ªå¯¹è±¡ï¼Œ{len(relations)} ä¸ªå…³ç³»")
    return detections, relations

def visualize_results(image, detections, relations, output_path):
    """å¯è§†åŒ–æ£€æµ‹å’Œå…³ç³»ç»“æœ"""
    print(f"ğŸ”„ ç”Ÿæˆå¯è§†åŒ–ç»“æœ")
    
    fig, ax = plt.subplots(1, 1, figsize=(12, 8))
    ax.imshow(image)
    
    # ç»˜åˆ¶è¾¹ç•Œæ¡†å’Œæ ‡ç­¾
    colors = plt.cm.Set3(np.linspace(0, 1, len(detections)))
    
    for i, det in enumerate(detections):
        bbox = det['bbox']
        x1, y1, x2, y2 = bbox
        
        # ç»˜åˆ¶è¾¹ç•Œæ¡†
        rect = patches.Rectangle((x1, y1), x2-x1, y2-y1, 
                               linewidth=2, edgecolor=colors[i], facecolor='none')
        ax.add_patch(rect)
        
        # æ·»åŠ æ ‡ç­¾
        label_text = f"{det['class_name']}: {det['confidence']:.2f}"
        ax.text(x1, y1-5, label_text, fontsize=10, 
               bbox=dict(boxstyle="round,pad=0.3", facecolor=colors[i], alpha=0.7))
    
    # ç»˜åˆ¶å…³ç³»è¿çº¿
    for rel in relations:
        if rel['subject'] < len(detections) and rel['object'] < len(detections):
            subj_bbox = detections[rel['subject']]['bbox']
            obj_bbox = detections[rel['object']]['bbox']
            
            # è®¡ç®—è¾¹ç•Œæ¡†ä¸­å¿ƒç‚¹
            subj_center = [(subj_bbox[0] + subj_bbox[2])/2, (subj_bbox[1] + subj_bbox[3])/2]
            obj_center = [(obj_bbox[0] + obj_bbox[2])/2, (obj_bbox[1] + obj_bbox[3])/2]
            
            # ç»˜åˆ¶è¿çº¿
            ax.annotate('', xy=obj_center, xytext=subj_center,
                       arrowprops=dict(arrowstyle='->', color='red', lw=2, alpha=0.7))
            
            # æ·»åŠ å…³ç³»æ ‡ç­¾
            mid_x = (subj_center[0] + obj_center[0]) / 2
            mid_y = (subj_center[1] + obj_center[1]) / 2
            ax.text(mid_x, mid_y, rel['predicate_name'], 
                   fontsize=8, ha='center', va='center',
                   bbox=dict(boxstyle="round,pad=0.2", facecolor='yellow', alpha=0.7))
    
    ax.set_title('PID Scene Graph Detection Results', fontsize=14)
    ax.axis('off')
    
    plt.tight_layout()
    plt.savefig(output_path, dpi=300, bbox_inches='tight')
    plt.close()
    
    print(f"âœ… å¯è§†åŒ–ç»“æœä¿å­˜åˆ°: {output_path}")

def save_results(detections, relations, output_path):
    """ä¿å­˜æ£€æµ‹ç»“æœä¸ºJSONæ ¼å¼"""
    
    # è½¬æ¢numpyæ•°ç»„ä¸ºåˆ—è¡¨ï¼Œç¡®ä¿JSONå¯åºåˆ—åŒ–
    def convert_to_serializable(obj):
        if isinstance(obj, np.ndarray):
            return obj.tolist()
        elif isinstance(obj, (np.integer, np.floating)):
            return obj.item()
        elif isinstance(obj, dict):
            return {k: convert_to_serializable(v) for k, v in obj.items()}
        elif isinstance(obj, list):
            return [convert_to_serializable(item) for item in obj]
        else:
            return obj
    
    results = {
        'detections': convert_to_serializable(detections),
        'relations': convert_to_serializable(relations),
        'num_detections': len(detections),
        'num_relations': len(relations)
    }
    
    with open(output_path, 'w') as f:
        json.dump(results, f, indent=2, ensure_ascii=False)
    
    print(f"âœ… æ£€æµ‹ç»“æœä¿å­˜åˆ°: {output_path}")

def main():
    parser = argparse.ArgumentParser('PID Inference', parents=[get_args_parser()])
    args = parser.parse_args()
    
    print("ğŸš€ å¼€å§‹PIDåœºæ™¯å›¾ç”Ÿæˆæ¨ç†")
    print(f"   æ¨¡å‹è·¯å¾„: {args.model_path}")
    print(f"   å›¾åƒè·¯å¾„: {args.image_path}")
    print(f"   è¾“å‡ºç›®å½•: {args.output_dir}")
    print(f"   è®¾å¤‡: {args.device}")
    
    # åˆ›å»ºè¾“å‡ºç›®å½•
    output_dir = Path(args.output_dir)
    output_dir.mkdir(exist_ok=True)
    
    # åŠ è½½æ¨¡å‹
    model, device = load_model(args)
    
    # é¢„å¤„ç†å›¾åƒ
    samples, orig_size, orig_image = preprocess_image(args.image_path)
    samples = samples.to(device)
    
    # æ¨ç†
    print("ğŸ”„ æ‰§è¡Œæ¨¡å‹æ¨ç†...")
    with torch.no_grad():
        outputs = model(samples)
    
    # åå¤„ç†
    detections, relations = postprocess_predictions(
        outputs, orig_size, 
        confidence_threshold=args.confidence_threshold,
        max_detections=args.max_detections
    )
    
    # ä¿å­˜å’Œå¯è§†åŒ–ç»“æœ
    image_name = Path(args.image_path).stem
    
    # ä¿å­˜JSONç»“æœ
    json_path = output_dir / f"{image_name}_results.json"
    save_results(detections, relations, json_path)
    
    # ç”Ÿæˆå¯è§†åŒ–
    viz_path = output_dir / f"{image_name}_visualization.png"
    visualize_results(orig_image, detections, relations, viz_path)
    
    # æ‰“å°æ‘˜è¦
    print("\nğŸ“Š æ¨ç†ç»“æœæ‘˜è¦:")
    print(f"   æ£€æµ‹åˆ°å¯¹è±¡æ•°é‡: {len(detections)}")
    print(f"   æ£€æµ‹åˆ°å…³ç³»æ•°é‡: {len(relations)}")
    
    if detections:
        print("\nğŸ¯ æ£€æµ‹åˆ°çš„å¯¹è±¡:")
        for i, det in enumerate(detections[:10]):  # åªæ˜¾ç¤ºå‰10ä¸ª
            print(f"   {i+1}. {det['class_name']} (ç½®ä¿¡åº¦: {det['confidence']:.3f})")
        if len(detections) > 10:
            print(f"   ... è¿˜æœ‰ {len(detections)-10} ä¸ªå¯¹è±¡")
    
    if relations:
        print("\nğŸ”— æ£€æµ‹åˆ°çš„å…³ç³»:")
        for i, rel in enumerate(relations[:10]):  # åªæ˜¾ç¤ºå‰10ä¸ª
            if rel['subject'] < len(detections) and rel['object'] < len(detections):
                subj_name = detections[rel['subject']]['class_name']
                obj_name = detections[rel['object']]['class_name']
                print(f"   {i+1}. {subj_name} -> {rel['predicate_name']} -> {obj_name} (ç½®ä¿¡åº¦: {rel['confidence']:.3f})")
        if len(relations) > 10:
            print(f"   ... è¿˜æœ‰ {len(relations)-10} ä¸ªå…³ç³»")
    
    print(f"\nâœ… æ¨ç†å®Œæˆï¼ç»“æœä¿å­˜åœ¨: {output_dir}")

if __name__ == '__main__':
    main()
