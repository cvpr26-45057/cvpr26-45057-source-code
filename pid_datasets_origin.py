import os
import sys
import torchvision
import torchvision.transforms as T
import torchvision.datasets as datasets
import torch
import numpy as np
from torch.utils.data import Dataset, DataLoader, Subset
from PIL import Image
import networkx as nx
import random
import tempfile
import json
from pycocotools.coco import COCO

class PIDGraphDatasetExtractor:
    def __init__(self, base_dir):
        self.base_dir = base_dir
        self.data_info = []
        

    def extract(self):
        # pattern = os.path.join(self.base_dir, '**', '*.graphml')
        # files = set(glob.glob(pattern, recursive=True))
        # for graphml_path in files:
        #     self._process_graphml(graphml_path)
        # print(f"Found {len(files)} GraphML files in {self.base_dir}")

        # with open(os.path.join(self.output_dir, 'dataset_info.json'), 'w') as f:
        #     json.dump(self.data_info, f, indent=4)
        paired_files = []
        for root, dirs, files in os.walk(self.base_dir):
            # æŒ‰æ–‡ä»¶ååˆ†ç»„
            file_groups = {}
            
            for file in files:
                base_name = os.path.splitext(file)[0]
                ext = os.path.splitext(file)[1].lower()
                
                if base_name not in file_groups:
                    file_groups[base_name] = {}
                
                if ext in ['.png', '.jpg', '.jpeg']:
                    file_groups[base_name]['image'] = os.path.join(root, file)
                elif ext == '.graphml':
                    file_groups[base_name]['annotation'] = os.path.join(root, file)
            
            # æ‰¾åˆ°å®Œæ•´é…å¯¹çš„æ–‡ä»¶
            for base_name, files_dict in file_groups.items():
                if 'image' in files_dict and 'annotation' in files_dict:
                    paired_files.append({
                        'id': base_name,
                        'image_path': files_dict['image'],
                        'annotation_path': files_dict['annotation'],
                        'relative_path': os.path.relpath(root, self.base_dir)
                    })

        print(f"åœ¨ {self.base_dir} ä¸­æ‰¾åˆ° {len(paired_files)} å¯¹é…å¯¹æ–‡ä»¶")
        return paired_files
    
    def extract_annotations_from_graphml(self, graphml_path):
        """ä» GraphML æ–‡ä»¶ä¸­æå–æ ‡æ³¨ä¿¡æ¯"""
        try:
            graph = nx.read_graphml(graphml_path)
            
            # æå–èŠ‚ç‚¹ï¼ˆå¯¹è±¡ï¼‰ä¿¡æ¯
            objects = []
            for node_id, node_attrs in graph.nodes(data=True):
                # æ£€æŸ¥æ˜¯å¦æœ‰è¾¹ç•Œæ¡†ä¿¡æ¯
                bbox_keys = ['xmin', 'ymin', 'xmax', 'ymax']
                if all(key in node_attrs for key in bbox_keys):
                    try:
                        bbox = [float(node_attrs[key]) for key in bbox_keys]
                        
                        obj_info = {
                            'id': node_id,
                            'bbox': bbox,  # [xmin, ymin, xmax, ymax]
                            'label': node_attrs.get('label', 'unknown'),
                            'category': node_attrs.get('category', 'object'),
                            'attributes': {k: v for k, v in node_attrs.items() 
                                         if k not in bbox_keys + ['label', 'category']}
                        }
                        objects.append(obj_info)
                    except (ValueError, TypeError):
                        continue
            
            # æå–è¾¹ï¼ˆå…³ç³»ï¼‰ä¿¡æ¯
            relations = []
            for src, dst, edge_attrs in graph.edges(data=True):
                rel_info = {
                    'subject': src,
                    'object': dst,
                    'predicate': edge_attrs.get('edge_label'),
                    'attributes': dict(edge_attrs)
                }
                relations.append(rel_info)
            
            return {
                'objects': objects,
                'relations': relations,
                'num_objects': len(objects),
                'num_relations': len(relations)
            }
            
        except Exception as e:
            print(f"è§£æ GraphML æ–‡ä»¶å¤±è´¥ {graphml_path}: {e}")
            return None


class PIDGraphDataset(Dataset):
    def __init__(self, complete_img=False, min_objects=1, transform=None):
        if complete_img==False:
            self.base_dir = 'PID2Graph/patched'
        else:
            self.base_dir = 'PID2Graph/Complete'
        self.transform = transform
        # 1. åŠ è½½å…³ç³»ç±»åˆ«é…ç½®
        self.rel_categories = ['__background__',      # 0 - èƒŒæ™¯ç±»åˆ«
                               'solid',            # 1 - å®çº¿è¿æ¥
                               'non-solid',       # 2 - è™šçº¿è¿æ¥
                               ]
        
        # 2. åˆ›å»ºå…³ç³»ç±»åˆ«æ˜ å°„
        self.relation_label_to_idx = {rel: idx for idx, rel in enumerate(self.rel_categories)}
        self.idx_to_relation_label = {idx: rel for idx, rel in enumerate(self.rel_categories)}
        
        # 3. åŠ è½½å¯¹è±¡ç±»åˆ« (å¦‚æœéœ€è¦)
        self.obj_categories = [
            '__background__',     # 0 - èƒŒæ™¯
            'pump',              # 1 - æ³µ
            'valve',             # 2 - é˜€é—¨
            'tank',              # 3 - å‚¨ç½
            'pipe',              # 4 - ç®¡é“
            'sensor',            # 5 - ä¼ æ„Ÿå™¨
            'motor',             # 6 - ç”µæœº
            'heat_exchanger',    # 7 - æ¢çƒ­å™¨
            'compressor',        # 8 - å‹ç¼©æœº
            'filter',            # 9 - è¿‡æ»¤å™¨
            'control_valve',     # 10 - è°ƒèŠ‚é˜€
            'pressure_vessel',   # 11 - å‹åŠ›å®¹å™¨
            'instrument',        # 12 - ä»ªè¡¨
        ]
        
        self.label_to_idx = {
            obj: idx for idx, obj in enumerate(self.obj_categories)
        }
        
        self.idx_to_label = {
            idx: obj for idx, obj in enumerate(self.obj_categories)
        }
        # æå–æ•°æ®é›†ä¿¡æ¯
        self.extractor = PIDGraphDatasetExtractor(self.base_dir)
        self.paired_files = self.extractor.extract()
        # éªŒè¯å¹¶è¿‡æ»¤æœ‰æ•ˆæ ·æœ¬
        self.valid_samples = []
        print("éªŒè¯æ ·æœ¬...")
        
        for i, pair in enumerate(self.paired_files):
            annotations = self.extractor.extract_annotations_from_graphml(pair['annotation_path'])
            
            if annotations and annotations['num_objects'] >= min_objects:
                pair['annotations'] = annotations
                self.valid_samples.append(pair)
        
        print(f"\næ•°æ®é›†åŠ è½½å®Œæˆ:")
        print(f"  åŸå§‹é…å¯¹: {len(self.paired_files)}")
        print(f"  æœ‰æ•ˆæ ·æœ¬: {len(self.valid_samples)}")
        self._create_label_mappings()

    def _create_label_mappings(self):
        """åˆ›å»ºæ ‡ç­¾æ˜ å°„"""
        all_object_labels = set()
        all_relation_labels = set()
        
        for sample in self.valid_samples:
            for obj in sample['annotations']['objects']:
                all_object_labels.add(obj['label'])
            for rel in sample['annotations']['relations']:
                all_relation_labels.add(rel['predicate'])
        
        # å¯¹è±¡æ ‡ç­¾æ˜ å°„ï¼ˆæ·»åŠ èƒŒæ™¯ç±»ï¼‰
        self.object_classes = ['__background__'] + sorted(list(all_object_labels))
        self.object_label_to_idx = {label: idx for idx, label in enumerate(self.object_classes)}
        self.idx_to_object_label = {idx: label for label, idx in self.object_label_to_idx.items()}
        self.num_object_classes = len(self.object_classes)
        
        # å…³ç³»æ ‡ç­¾æ˜ å°„ï¼ˆæ·»åŠ èƒŒæ™¯ç±»ï¼‰
        self.relation_classes = ['__background__'] + sorted(list(all_relation_labels))
        self.relation_label_to_idx = {label: idx for idx, label in enumerate(self.relation_classes)}
        self.idx_to_relation_label = {idx: label for label, idx in self.relation_label_to_idx.items()}
        self.num_relation_classes = len(self.relation_classes)
        
        print(f"  å¯¹è±¡ç±»åˆ«: {self.num_object_classes} ä¸ª")
        print(f"  å…³ç³»ç±»åˆ«: {self.num_relation_classes} ä¸ª")
    
    def __len__(self):
        return len(self.valid_samples)
    
    def __getitem__(self, idx):
        """è¿”å›å›¾åƒã€ç›®æ ‡æ ‡æ³¨å’Œæ ·æœ¬ä¿¡æ¯"""
        sample = self.valid_samples[idx]
        
        # 1. åŠ è½½å›¾åƒ
        image = Image.open(sample['image_path']).convert('RGB')
        orig_size = torch.tensor(image.size)  # (width, height)
        
        # 2. å¤„ç†æ ‡æ³¨
        annotations = sample['annotations']
        
        # æå–å¯¹è±¡ä¿¡æ¯
        boxes = []
        labels = []
        object_ids = []
        
        for obj in annotations['objects']:
            boxes.append(obj['bbox'])  # [xmin, ymin, xmax, ymax]
            # è·å–æ ‡ç­¾ç´¢å¼•ï¼ˆè·³è¿‡èƒŒæ™¯ç±»ï¼Œä»1å¼€å§‹ï¼‰
            label_idx = self.object_label_to_idx.get(obj['label'], 1)  # é»˜è®¤ä¸ºç¬¬ä¸€ä¸ªçœŸå®ç±»åˆ«
            labels.append(label_idx)
            object_ids.append(obj['id'])
        
        # æå–å…³ç³»ä¿¡æ¯
        relations = []
        relation_labels = []
        rel_annotations = []
        for rel in annotations['relations']:
            try:
                # æ‰¾åˆ°ä¸»ä½“å’Œå®¢ä½“åœ¨å¯¹è±¡åˆ—è¡¨ä¸­çš„ç´¢å¼•
                sub_idx = object_ids.index(rel['subject'])
                obj_idx = object_ids.index(rel['object'])
                rel_label = self.relation_label_to_idx.get(rel['predicate'], 1)
                if len([sub_idx, obj_idx]) > 1:
                    # relations.append([sub_idx, obj_idx])  # [subject_idx, object_idx]
                    # relation_labels.append(rel_label)
                    rel_annotations.append([sub_idx, obj_idx, rel_label])
                else:
                    print(f"Warning: Relation {rel['predicate']} between {rel['subject']} and {rel['object']} not found in objects.")
                    rel_annotations.append([0, 0, 0])
            except ValueError:
                # è·³è¿‡æ‰¾ä¸åˆ°å¯¹åº”å¯¹è±¡çš„å…³ç³»
                continue
        # rel_annotations = torch.cat([relations, relation_labels.unsqueeze(1)], dim=1)
        
        # 3. è½¬æ¢ä¸ºå¼ é‡
        if len(boxes) > 0:
            boxes = torch.as_tensor(boxes, dtype=torch.float32)
            labels = torch.as_tensor(labels, dtype=torch.int64)
        else:
            # å¤„ç†æ²¡æœ‰å¯¹è±¡çš„æƒ…å†µ
            boxes = torch.zeros((0, 4), dtype=torch.float32)
            labels = torch.zeros((0,), dtype=torch.int64)
        
        if len(relations) > 0:
            relations = torch.as_tensor(relations, dtype=torch.int64)
            relation_labels = torch.as_tensor(relation_labels, dtype=torch.int64)
        else:
            relations = torch.zeros((0, 2), dtype=torch.int64)
            relation_labels = torch.zeros((0,), dtype=torch.int64)
        
        # 4. æ„å»ºç›®æ ‡å­—å…¸
        target = {
            'boxes': boxes,                    # [N, 4] è¾¹ç•Œæ¡†
            'labels': labels,                  # [N] å¯¹è±¡æ ‡ç­¾
            'image_id': torch.tensor([idx+ 1], dtype=torch.int64),   # [1] å›¾åƒID
            'orig_size': orig_size,            # [2] åŸå§‹å›¾åƒå°ºå¯¸ [H, W] - æ–°å¢
            # 'relations': relations,            # [M, 2] å…³ç³»å¯¹ç´¢å¼•
            # 'relation_labels': relation_labels, # [M] å…³ç³»æ ‡ç­¾
            'rel_annotations': torch.tensor(rel_annotations, dtype=torch.int64).view(-1, 3),  # [M, 3] - [subject_idx, object_idx, relation_label]
            'size': orig_size,              # [2] å›¾åƒå°ºå¯¸ [H, W]
            # 'num_objects': len(boxes),         # å¯¹è±¡æ•°é‡
            # 'num_relations': len(relations),   # å…³ç³»æ•°é‡
            # 'sample_id': sample['id']            # æ ·æœ¬IDå­—ç¬¦ä¸²
            'iscrowd': torch.zeros(len(boxes), dtype=torch.int64)
        }
        
        # 5. åº”ç”¨å›¾åƒå˜æ¢
        if self.transform:
            image = self.transform(image)
        
        return image, target
    
    def get_dataset_info(self):
        """è·å–æ•°æ®é›†ä¿¡æ¯"""
        return {
            'total_samples': len(self.valid_samples),
            'object_classes': self.object_classes,
            'relation_classes': self.relation_classes,
            'num_object_classes': self.num_object_classes,
            'num_relation_classes': self.num_relation_classes
        }
    
    def get_sample_by_id(self, sample_id):
        """æ ¹æ®æ ·æœ¬IDè·å–æ ·æœ¬"""
        for sample in self.valid_samples:
            if sample['id'] == sample_id:
                return sample
        return None


class CocoCompatibleDataset(torchvision.datasets.CocoDetection):
    """COCO å…¼å®¹çš„æ•°æ®é›†åŒ…è£…ç±»"""
    
    def __init__(self, original_dataset, indices=None):
        # ä¸è°ƒç”¨çˆ¶ç±»çš„ __init__ï¼Œå› ä¸ºæˆ‘ä»¬è¦è‡ªå®šä¹‰è¡Œä¸º
        self.original_dataset = original_dataset
        self.indices = indices if indices is not None else list(range(len(original_dataset)))
        
        # ğŸ”‘ åˆ›å»º COCO API å¯¹è±¡
        self.coco = self._create_coco_api()
        
        # å¤åˆ¶åŸæ•°æ®é›†çš„å±æ€§
        self.rel_categories = getattr(original_dataset, 'rel_categories', ['__background__'])
        self.obj_categories = getattr(original_dataset, 'obj_categories', ['__background__'])
        
    def __len__(self):
        return len(self.indices)
    
    def __getitem__(self, idx):
        # è·å–çœŸå®çš„ç´¢å¼•
        real_idx = self.indices[idx]
        return self.original_dataset[real_idx]
    
    def _create_coco_api(self):
        """åˆ›å»º COCO API å¯¹è±¡"""
        try:
            # æ„å»º COCO æ ¼å¼æ•°æ®
            images = []
            annotations = []
            categories = []
            
            # è·å–ç±»åˆ«ä¿¡æ¯
            obj_categories = getattr(self.original_dataset, 'obj_categories', ['__background__', 'object'])
            
            # åˆ›å»ºç±»åˆ«ï¼ˆè·³è¿‡èƒŒæ™¯ç±»ï¼‰
            for i, cat_name in enumerate(obj_categories):
                if i == 0:  # è·³è¿‡èƒŒæ™¯ç±»
                    continue
                categories.append({
                    "id": i,
                    "name": cat_name,
                    "supercategory": "thing"
                })
            
            if not categories:
                categories.append({"id": 1, "name": "object", "supercategory": "thing"})
            
            # å¤„ç†æ ·æœ¬ï¼ˆé™åˆ¶æ•°é‡é¿å…å†…å­˜é—®é¢˜ï¼‰
            num_samples = min(len(self.indices), 50)
            ann_id = 1
            
            for i in range(num_samples):
                try:
                    real_idx = self.indices[i]
                    image, target = self.original_dataset[real_idx]
                    
                    # å›¾åƒä¿¡æ¯
                    if 'orig_size' in target:
                        h, w = target['orig_size'].tolist()
                    else:
                        h, w = 800, 800
                    
                    image_info = {
                        "id": i + 1,
                        "width": w,
                        "height": h,
                        "file_name": f"image_{real_idx}.jpg"
                    }
                    images.append(image_info)
                    
                    # æ ‡æ³¨ä¿¡æ¯
                    if 'boxes' in target and 'labels' in target:
                        boxes = target['boxes']
                        labels = target['labels']
                        
                        for box, label in zip(boxes, labels):
                            if len(box) == 4 and label.item() > 0:
                                x1, y1, x2, y2 = box.tolist()
                                annotation = {
                                    "id": ann_id,
                                    "image_id": i + 1,
                                    "category_id": int(label.item()),
                                    "bbox": [x1, y1, x2 - x1, y2 - y1],
                                    "area": (x2 - x1) * (y2 - y1),
                                    "iscrowd": 0
                                }
                                annotations.append(annotation)
                                ann_id += 1
                except Exception as e:
                    print(f"å¤„ç†æ ·æœ¬ {i} æ—¶å‡ºé”™: {e}")
                    continue
            
            # ç¡®ä¿è‡³å°‘æœ‰ä¸€ä¸ªå›¾åƒå’Œæ ‡æ³¨
            if not images:
                images.append({"id": 1, "width": 800, "height": 800, "file_name": "dummy.jpg"})
            
            if not annotations:
                annotations.append({
                    "id": 1, "image_id": 1, "category_id": 1,
                    "bbox": [100, 100, 100, 100], "area": 10000, "iscrowd": 0
                })
            
            # åˆ›å»º COCO æ•°æ®
            coco_data = {
                "images": images,
                "annotations": annotations,
                "categories": categories,
                "info": {"description": "PID Dataset in COCO format", "version": "1.0"}
            }
            # print('cocoæ•°æ®: ', coco_data.get('images', []))
            
            # åˆ›å»ºä¸´æ—¶æ–‡ä»¶
            temp_file = tempfile.NamedTemporaryFile(mode='w', suffix='.json', delete=False)
            json.dump(coco_data, temp_file, indent=2)
            temp_file.close()
            
            try:
                coco_api = COCO(temp_file.name)
                os.unlink(temp_file.name)
                print(f"âœ… åˆ›å»º COCO API: {len(images)} å›¾åƒ, {len(annotations)} æ ‡æ³¨")
                return coco_api
            except Exception as e:
                print(f"âŒ COCO API åˆ›å»ºå¤±è´¥: {e}")
                if os.path.exists(temp_file.name):
                    os.unlink(temp_file.name)
                return self._create_minimal_coco_api()
                
        except Exception as e:
            print(f"âŒ _create_coco_api å‡ºé”™: {e}")
            return self._create_minimal_coco_api()
    
    def _create_minimal_coco_api(self):
        """åˆ›å»ºæœ€å°çš„ COCO API"""
        dummy_data = {
            "images": [{"id": 1, "width": 800, "height": 800, "file_name": "dummy.jpg"}],
            "annotations": [{"id": 1, "image_id": 1, "category_id": 1, "bbox": [100, 100, 100, 100], "area": 10000, "iscrowd": 0}],
            "categories": [{"id": 1, "name": "object", "supercategory": "thing"}]
        }
        
        temp_file = tempfile.NamedTemporaryFile(mode='w', suffix='.json', delete=False)
        json.dump(dummy_data, temp_file)
        temp_file.close()
        
        try:
            coco_api = COCO(temp_file.name)
            os.unlink(temp_file.name)
            return coco_api
        except:
            if os.path.exists(temp_file.name):
                os.unlink(temp_file.name)
            return None

def create_transforms(train=True):
    """åˆ›å»ºæ•°æ®å˜æ¢"""
    if train:
        return T.Compose([
            T.Resize((800, 800)),
            T.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2),
            T.ToTensor(),
            T.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])
        ])
    else:
        return T.Compose([
            T.Resize((800, 800)),
            T.ToTensor(),
            T.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])
        ])


def collate_fn(batch):
    """è‡ªå®šä¹‰ collate å‡½æ•°å¤„ç†ä¸åŒæ•°é‡çš„å¯¹è±¡å’Œå…³ç³»"""
    images = []
    targets = []
    
    for image, target in batch:
        images.append(image)
        targets.append(target)
    
    # å°†å›¾åƒå †å æˆæ‰¹æ¬¡
    images = torch.stack(images, dim=0)
    
    return images, targets


def create_datasets(complete_img=False, batch_size=4, shuffle=True, num_workers=2, train=True, split_ratio=0.8, seed=42):
    """åˆ›å»º DataLoader"""
    transform = create_transforms(train=train)
    
    dataset = PIDGraphDataset(
        complete_img=complete_img,
        min_objects=1,
        transform=transform
    )
    # åˆ’åˆ†è®­ç»ƒé›†å’ŒéªŒè¯é›†
    total_size = len(dataset)
    train_size = int(total_size * split_ratio)
    val_size = total_size - train_size
    print(f"\n=== æ•°æ®é›†åˆ’åˆ† ===")
    print(f"æ€»æ ·æœ¬æ•°: {total_size}")
    print(f"è®­ç»ƒé›†å¤§å°: {train_size} ({split_ratio:.1%})")
    print(f"éªŒè¯é›†å¤§å°: {val_size} ({1-split_ratio:.1%})")
    # è®¾ç½®éšæœºç§å­ç¡®ä¿å¯é‡å¤æ€§
    random.seed(seed)
    torch.manual_seed(seed)
    
    # ç”Ÿæˆéšæœºç´¢å¼•
    indices = list(range(total_size))
    random.shuffle(indices)
    
    train_indices = indices[:train_size]
    val_indices = indices[train_size:]
    
    # åˆ›å»ºå­æ•°æ®é›†
    train_dataset = Subset(dataset, train_indices)
    val_dataset = Subset(dataset, val_indices)
    # ä¸ºè®­ç»ƒé›†å’ŒéªŒè¯é›†è®¾ç½®ä¸åŒçš„å˜æ¢
    # ğŸ”‘ æ‰‹åŠ¨ç»™ Subset æ·»åŠ å¿…è¦çš„å±æ€§
    train_dataset.rel_categories = dataset.rel_categories
    train_dataset.obj_categories = dataset.obj_categories
    train_dataset.relation_label_to_idx = dataset.relation_label_to_idx
    train_dataset.idx_to_relation_label = dataset.idx_to_relation_label
    train_dataset.label_to_idx = dataset.label_to_idx
    train_dataset.idx_to_label = dataset.idx_to_label
    train_dataset.object_classes = dataset.object_classes
    train_dataset.relation_classes = dataset.relation_classes
    train_dataset.num_object_classes = dataset.num_object_classes
    train_dataset.num_relation_classes = dataset.num_relation_classes
    
    val_dataset.rel_categories = dataset.rel_categories
    val_dataset.obj_categories = dataset.obj_categories
    val_dataset.relation_label_to_idx = dataset.relation_label_to_idx
    val_dataset.idx_to_relation_label = dataset.idx_to_relation_label
    val_dataset.label_to_idx = dataset.label_to_idx
    val_dataset.idx_to_label = dataset.idx_to_label
    val_dataset.object_classes = dataset.object_classes
    val_dataset.relation_classes = dataset.relation_classes
    val_dataset.num_object_classes = dataset.num_object_classes
    val_dataset.num_relation_classes = dataset.num_relation_classes

    train_dataset = CocoCompatibleDataset(dataset, train_indices)
    val_dataset = CocoCompatibleDataset(dataset, val_indices)
    print(f"âœ… åˆ›å»º COCO å…¼å®¹æ•°æ®é›†æˆåŠŸ")
    print(f"è®­ç»ƒé›†: {len(train_dataset)}, éªŒè¯é›†: {len(val_dataset)}")
    print(f"è®­ç»ƒé›†ç±»å‹æ£€æŸ¥: {isinstance(train_dataset, torchvision.datasets.CocoDetection)}")
    print(f"éªŒè¯é›†ç±»å‹æ£€æŸ¥: {isinstance(val_dataset, torchvision.datasets.CocoDetection)}")
    
    return train_dataset,  val_dataset


# å¯è§†åŒ–å‡½æ•°
def visualize_sample(dataset, idx, save_path=None):
    """å¯è§†åŒ–æ ·æœ¬"""
    import matplotlib.pyplot as plt
    import matplotlib.patches as patches
    
    # è·å–åŸå§‹æ•°æ®ï¼ˆä¸ç»è¿‡å˜æ¢ï¼‰
    sample = dataset[idx]
    image_tensor, targets = dataset[idx]
            
    # ğŸ”‘ å¦‚æœæ˜¯å¼ é‡ï¼Œè½¬æ¢å› PIL å›¾åƒæ ¼å¼
    if isinstance(image_tensor, torch.Tensor):
        # åå½’ä¸€åŒ–
        if image_tensor.shape[0] == 3:  # (C, H, W)
            # å‡è®¾ä½¿ç”¨äº†æ ‡å‡†çš„ ImageNet å½’ä¸€åŒ–
            mean = torch.tensor([0.485, 0.456, 0.406]).view(3, 1, 1)
            std = torch.tensor([0.229, 0.224, 0.225]).view(3, 1, 1)
                    
            # åå½’ä¸€åŒ–
            image_tensor = image_tensor * std + mean
            image_tensor = torch.clamp(image_tensor, 0, 1)
                    
            # è½¬æ¢ä¸º (H, W, C) æ ¼å¼
            image_np = image_tensor.permute(1, 2, 0).numpy()
                    
            # è½¬æ¢ä¸º PIL å›¾åƒ
            image = Image.fromarray((image_np * 255).astype(np.uint8))
        else:
            print(f"âŒ æœªçŸ¥çš„å›¾åƒå¼ é‡å½¢çŠ¶: {image_tensor.shape}")
            return
    else:
        image = image_tensor
    
    fig, ax = plt.subplots(1, 1, figsize=(12, 8))
    ax.imshow(image)
    # print(targets)
    # ç»˜åˆ¶å¯¹è±¡è¾¹ç•Œæ¡†
    colors = plt.cm.Set3(np.linspace(0, 1, max(targets['boxes'].shape[0], 1)))
    object_positions = {}

    for i, box in enumerate(targets['boxes']):
        bbox = box  # [xmin, ymin, xmax, ymax]
        print('box: ', box)
        print('bbox: ', bbox)
        # ç»˜åˆ¶è¾¹ç•Œæ¡†
        rect = patches.Rectangle(
            (bbox[0], bbox[1]), 
            bbox[2] - bbox[0], 
            bbox[3] - bbox[1],
            linewidth=2, 
            edgecolor=colors[i % len(colors)], 
            facecolor='none'
        )
        ax.add_patch(rect)
        
        # æ·»åŠ æ ‡ç­¾
        ax.text(
            bbox[0], bbox[1] - 10, 
            f"{box['label']} ({box['id']})",
            fontsize=10, 
            color=colors[i % len(colors)],
            bbox=dict(boxstyle="round,pad=0.3", facecolor='white', alpha=0.7)
        )
        
        # è®°å½•å¯¹è±¡ä¸­å¿ƒä½ç½®ç”¨äºç»˜åˆ¶å…³ç³»
        object_positions[i] = ((bbox[0] + bbox[2]) / 2, (bbox[1] + bbox[3]) / 2)
    
    # ç»˜åˆ¶å…³ç³»
    for rel in targets['relations']:
        if rel['subject'] in object_positions and rel['object'] in object_positions:
            sub_pos = object_positions[rel['subject']]
            obj_pos = object_positions[rel['object']]
            
            # ç»˜åˆ¶ç®­å¤´
            ax.annotate('', xy=obj_pos, xytext=sub_pos,
                       arrowprops=dict(arrowstyle='->', color='red', lw=2))
            
            # æ·»åŠ å…³ç³»æ ‡ç­¾
            mid_x = (sub_pos[0] + obj_pos[0]) / 2
            mid_y = (sub_pos[1] + obj_pos[1]) / 2
            ax.text(mid_x, mid_y, rel['predicate'], 
                   fontsize=8, color='red', 
                   bbox=dict(boxstyle="round,pad=0.2", facecolor='yellow', alpha=0.7))
    
    ax.set_title(f"æ ·æœ¬ {idx}: {sample['id']}\nå¯¹è±¡: {annotations['num_objects']}, å…³ç³»: {annotations['num_relations']}")
    ax.axis('off')
    
    if save_path:
        plt.savefig(save_path, bbox_inches='tight', dpi=150)
    plt.show()


if __name__ == "__main__":
    current_dir = os.path.dirname(os.path.abspath(__file__))
    project_root = os.path.dirname(current_dir)  # ä¸Šçº§ç›®å½• (RelTR-main)
    if project_root not in sys.path:
        sys.path.insert(0, project_root)
    # åˆ›å»ºæ•°æ®é›†å’Œæ•°æ®åŠ è½½å™¨
    dataset_train, dataset_val = create_datasets(
        complete_img=False,  # ä½¿ç”¨ patched æ•°æ®
        batch_size=2,
        shuffle=True,
        train=True
    )
    from torch.utils.data import DataLoader
    import util.misc as utils
    def simple_collate_fn(batch):
        """ç®€åŒ–çš„ collate å‡½æ•°"""
        images = []
        targets = []
        
        for item in batch:
            if item is not None and len(item) == 2:
                image, target = item
                images.append(image)
                targets.append(target)
        
        if len(images) == 0:
            return None
        
        # å°†å›¾åƒå †å æˆæ‰¹æ¬¡
        images = torch.stack(images, dim=0)
        
        return images, targets
    
    # ğŸ”‘ ä½¿ç”¨ç®€å•çš„ DataLoaderï¼Œä¸ä½¿ç”¨åˆ†å¸ƒå¼
    data_loader_train = DataLoader(
        dataset_train, 
        batch_size=2,
        shuffle=True,
        collate_fn=simple_collate_fn, 
        num_workers=0,  # è®¾ç½®ä¸º 0 é¿å…å¤šè¿›ç¨‹é—®é¢˜
        drop_last=True
    )
    
    # æµ‹è¯•æ•°æ®åŠ è½½å™¨
    print(f"\n=== æµ‹è¯• DataLoader ===")
    for batch_idx, (images, targets) in enumerate(data_loader_train):
        print(f"Batch {batch_idx + 1}:")
        print(f"  å›¾åƒå½¢çŠ¶: {images.shape}")
        print(f"  æ‰¹æ¬¡å¤§å°: {len(targets)}")
        
        for i, target in enumerate(targets):
            print(f"    æ ·æœ¬ {i}: ID={target['image_id']}")
            print(f"      è¾¹ç•Œæ¡†å½¢çŠ¶: {target['boxes'].shape}")
            print(f"      æ ‡ç­¾: {target['labels'].tolist()}")
            print(f"      å…³ç³»: {target['rel_annotations'].tolist()}")

        
        if batch_idx >= 1:  # åªæ˜¾ç¤ºå‰2ä¸ªbatch
            break
    
    # å¯è§†åŒ–æ ·æœ¬
    print(f"\n=== å¯è§†åŒ–æ ·æœ¬ ===")
    for i in range(min(2, len(dataset_train))):
        visualize_sample(dataset_train, i)